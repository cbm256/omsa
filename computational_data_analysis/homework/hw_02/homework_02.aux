\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\@writefile{toc}{\contentsline {section}{\numberline {1}Image Compression Using Clustering}{1}{section.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1}General K-medoids implementation}{1}{subsection.1.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2}Comparison of Different K values and Distance Measures}{1}{subsection.1.2}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:k_3}{{1a}{1}{$k=3$\relax }{figure.caption.1}{}}
\newlabel{sub@fig:k_3}{{a}{1}{$k=3$\relax }{figure.caption.1}{}}
\newlabel{fig:k_15}{{1b}{1}{$k=15$\relax }{figure.caption.1}{}}
\newlabel{sub@fig:k_15}{{b}{1}{$k=15$\relax }{figure.caption.1}{}}
\newlabel{fig:k_25}{{1c}{1}{$k=25$\relax }{figure.caption.1}{}}
\newlabel{sub@fig:k_25}{{c}{1}{$k=25$\relax }{figure.caption.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Comparison of 3 different distance measures (Euclidean, trace class norm, and infinite norm) with 3 diffent values for K (3, 15, 25)\relax }}{1}{figure.caption.1}}
\newlabel{fig1}{{1}{1}{Comparison of 3 different distance measures (Euclidean, trace class norm, and infinite norm) with 3 diffent values for K (3, 15, 25)\relax }{figure.caption.1}{}}
\newlabel{fig:k_medoids_time}{{2a}{2}{$K-medoids$\relax }{figure.caption.2}{}}
\newlabel{sub@fig:k_medoids_time}{{a}{2}{$K-medoids$\relax }{figure.caption.2}{}}
\newlabel{fig:k_means_time}{{2b}{2}{$K-means$\relax }{figure.caption.2}{}}
\newlabel{sub@fig:k_means_time}{{b}{2}{$K-means$\relax }{figure.caption.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Time until convergence for different values of K\relax }}{2}{figure.caption.2}}
\newlabel{fig2}{{2}{2}{Time until convergence for different values of K\relax }{figure.caption.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3}Comparison with Different Initial Centroids}{2}{subsection.1.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.4}Comparison of K-means and K-medoids}{2}{subsection.1.4}}
\@writefile{toc}{\contentsline {section}{\numberline {2}Spectral Clustering}{2}{section.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Show that there are $m$ eigenvectors of $L$ corresponding to eigenvalue zero, and the indicator vectors of these components $I_{A_1} , . . . , I_{A_m}$ span the zero eigenspace.}{2}{subsection.2.1}}
\bibstyle{plain}
\bibdata{bibliography.bib}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Real Data: Political Blogs Dataset}{3}{subsection.2.2}}
\@writefile{toc}{\contentsline {section}{\numberline {3}PCA Food Consumption}{3}{section.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Find the first two principal directions, and plot them.}{3}{subsection.3.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Compute the reduced representation of the data point (which are sometimes called the principal components of the data). Draw a scatter plot of two-dimensional reduced representation for each country. Do you observe some pattern?}{3}{subsection.3.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces \it  The first two principal directions using eigendecomposition (numpy)\relax }}{4}{figure.caption.3}}
\newlabel{fig:dirs}{{3}{4}{\it The first two principal directions using eigendecomposition (numpy)\relax }{figure.caption.3}{}}
\newlabel{fig:pca_ed}{{4a}{5}{\it PCA using eigendecomposition (numpy)\relax }{figure.caption.4}{}}
\newlabel{sub@fig:pca_ed}{{a}{5}{\it PCA using eigendecomposition (numpy)\relax }{figure.caption.4}{}}
\newlabel{fig:pca_sp}{{4b}{5}{\it PCA using eigendecomposition (scipy)\relax }{figure.caption.4}{}}
\newlabel{sub@fig:pca_sp}{{b}{5}{\it PCA using eigendecomposition (scipy)\relax }{figure.caption.4}{}}
\newlabel{fig:pca_svd}{{4c}{5}{\it PCA using Singular Value Decomposition\relax }{figure.caption.4}{}}
\newlabel{sub@fig:pca_svd}{{c}{5}{\it PCA using Singular Value Decomposition\relax }{figure.caption.4}{}}
\newlabel{fig:pca_sk}{{4d}{5}{\it PCA using scikit-learn\relax }{figure.caption.4}{}}
\newlabel{sub@fig:pca_sk}{{d}{5}{\it PCA using scikit-learn\relax }{figure.caption.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces \it  Comparisons of Principal Component Analysis methods/libraries\relax }}{5}{figure.caption.4}}
\newlabel{fig3}{{4}{5}{\it Comparisons of Principal Component Analysis methods/libraries\relax }{figure.caption.4}{}}
