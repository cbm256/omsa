\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\@writefile{toc}{\contentsline {section}{\numberline {1}K-means}{1}{section.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1}Euclidean Distance}{1}{subsection.1.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces New cluster centers after one iteration of K-means using Euclidean distance\relax }}{1}{figure.caption.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2}Manhattan Distance}{1}{subsection.1.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces New cluster centers after one iteration of K-means using Manhattan distance\relax }}{2}{figure.caption.2}}
\@writefile{toc}{\contentsline {section}{\numberline {2}Spectral Clustering}{2}{section.2}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{spectral}{{3a}{2}{Cluster results from spectral clustering.\relax }{figure.caption.3}{}}
\newlabel{sub@spectral}{{a}{2}{Cluster results from spectral clustering.\relax }{figure.caption.3}{}}
\newlabel{kmeans}{{3b}{2}{Cluster results using K-means clustering.\relax }{figure.caption.3}{}}
\newlabel{sub@kmeans}{{b}{2}{Cluster results using K-means clustering.\relax }{figure.caption.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Two different clustering algorithm results on the same dataset. The spectral clustering used k nearest neighbors with 2 neighbors to build an adjacency matrix, then k-means on the results from eigendecomposition on the Laplacian matrix. K-means used Euclidean distance as measure. \relax }}{2}{figure.caption.3}}
\newlabel{clusterCompare}{{3}{2}{Two different clustering algorithm results on the same dataset. The spectral clustering used k nearest neighbors with 2 neighbors to build an adjacency matrix, then k-means on the results from eigendecomposition on the Laplacian matrix. K-means used Euclidean distance as measure. \relax }{figure.caption.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Principal Component Analysis}{3}{section.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Find the first principal direction.}{3}{subsection.3.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}When we reduce the dimensionality from 3 to 1 based on the principal direction you found above, what is the reconstruction error in terms of variance?}{3}{subsection.3.2}}
\bibstyle{plain}
\bibdata{bibliography.bib}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}You are given the following 2-D datasets, approximately draw the first and second principal directional on each plot.}{4}{subsection.3.3}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces The first 2 principal directions of 2 data clouds.\relax }}{4}{figure.caption.4}}
